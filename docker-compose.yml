version: '3.8'

services:
  spark-dev:
    build: 
      context: .
      dockerfile: Dockerfile  # Your existing Spark Dockerfile
    container_name: aviation_spark_dev

    user: root
    ports:
      - "9081:8080"
      - "9040:4040"
      - "7077:7077"
    volumes:
      - .:/opt/spark/work-dir
    environment:
      - SPARK_MODE=master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master

  postgres:
    image: postgres:13
    container_name: aviation_db
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow

  airflow:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: aviation_airflow
    depends_on:
      - postgres
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__WEBSERVER__AUTHENTICATE=False
      - AIRFLOW_ADMIN_USER=admin
      - AIRFLOW_ADMIN_PASSWORD=admin123
    volumes:
      - ./dags:/opt/airflow/dags
      - ./code:/opt/airflow/code
      - ./.env:/opt/airflow/.env 
    ports:
      - "9000:8080"
    # Entrypoint only handles Airflow setup, not system installs
    entrypoint: >
      /bin/bash -c "
      airflow db init &&
      airflow users create --username admin --password admin123 --firstname Gary --lastname User --role Admin --email admin@example.com || true &&
      airflow standalone"